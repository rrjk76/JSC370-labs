---
title: "Lab 10 - Trees, Bagging, RF, Boosting, XGBoost"
output: github_document
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(eval = T, include  = T, echo = T)
```

# Learning goals

- Perform classification and regression with tree-based methods in R
- Recognize that tree-based methods are capable of capturing non-linearities by splitting multiple times on the same variables
- Compare the performance of classification trees, bagging, random forests, and boosting for predicting heart disease based on the ``heart`` data.

# Lab description

For this lab we will be working with the `heart` dataset that you can download from [here](https://github.com/JSC370/JSC370-2025/blob/main/data/heart.csv)

# Deliverables

Questions 1-5 answered, pdf or html output uploaded to Quercus

### Setup packages

You should install and load `rpart` (trees), `randomForest` (random forest), `gbm` (gradient boosting) and `xgboost` (extreme gradient boosting).


```{r, eval = FALSE}
install.packages(c("rpart", "rpart.plot", "randomForest", "gbm", "xgboost"))
```

### Load packages and data
```{r warning=FALSE, message=FALSE}
library(tidyverse)
library(rpart)
library(rpart.plot)
library(randomForest)
library(gbm)
library(xgboost)
library(caret)

heart <- read_csv("https://raw.githubusercontent.com/JSC370/JSC370-2025/refs/heads/main/data/heart/heart.csv") |>
  mutate(
    AHD = 1 * (AHD == "Yes"),
    ChestPain = factor(ChestPain),
    Thal = factor(Thal)
  )

head(heart)
```


---


## Question 1: Trees
- Split the `heart` data into training and testing (70-30%)

```{r}

set.seed(1008913302)

train <- sample(1:nrow(heart), round(0.7 * nrow(heart)))

heart_train = heart[train,]
heart_test <- heart[-train, ]

```

- Fit a classification tree using rpart, plot the full tree. We are trying to predict AHD. Set minsplit = 10, minbucket = 3, and do 10 cross validations.

```{r}
heart_tree <- rpart(
  AHD~., data = heart_train,
  method = "class",
  control = list(minsplit = 10, minbucket = 3, cp = 0, xval = 10)
)
rpart.plot(heart_tree)
```

- Plot the complexity parameter table for an rpart fit and find the optimal cp

```{r}
plotcp(heart_tree)
printcp(heart_tree)
```

- Prune the tree

```{r}
optimalcp <- heart_tree$cptable[which.min(heart_tree$cptable[, "xerror"]), "CP"]

optimalcp
```


```{r}
heart_tree_prune<-prune(heart_tree, cp = optimalcp)
rpart.plot(heart_tree_prune)
```

- Compute the test misclassification error

```{r}
heart_pred<-predict(heart_tree_prune, heart_test)
heart_pred <- as.data.frame(heart_pred)
colnames(heart_pred) <- c("No", "Yes")
heart_pred$AHD <- ifelse(heart_pred$Yes>0.5, 1, 0)
```

```{r}
confmatrix_table <-table(true=heart_test$AHD, predicted = heart_pred$AHD)
confmatrix_table
```

```{r}
miscla_error <- ((confmatrix_table[1, 2] + confmatrix_table[2, 1])/nrow(heart_test))
miscla_error
```

- We can see that the misclassification error is around 19%.


- Fit the tree with the optimal complexity parameter to the full data (training + testing)

```{r}
heart_tree<-rpart(
  AHD~., data = heart,
  method = "class",
  control = list(minsplit = 10, minbucket = 3, cp = 0, xval = 10)
)
```

- Find the Out of Bag (OOB) error for tree

```{r}
heart_tree$cptable
```


---

## Question 2: Bagging, Random Forest

- Compare the performance of classification trees (above), bagging, random forests for predicting heart disease based on the ``heart`` data.

- Use the training and testing sets from above. Train each of the models on the training data and extract the cross-validation (or out-of-bag error for bagging and Random forest). 


- For bagging use ``randomForest`` with ``mtry`` equal to the number of features (all other parameters at their default values). Generate the variable importance plot using ``varImpPlot`` and extract variable importance from the ``randomForest`` fitted object using the ``importance`` function.

- For random forests use ``randomForest`` with the default parameters. Generate the variable importance plot using ``varImpPlot`` and extract variable importance from the ``randomForest`` fitted object using the ``importance`` function.


```{r message=FALSE, warning=FALSE}
heart_bag<-randomForest(as.factor(AHD)~., data = heart_train, mtry = 13, na.action =na.omit)
# oob error rate
sum(heart_bag$err.rate[, 1])

varImpPlot(heart_bag, 
           cex.lab = 1.5,
           cex.ax9s = 2,
           cex = 1.3,
           n.var = 13,
           main = "",
           pch = 10,
           col = "red")
```



# Question 3: Boosting

- For boosting use `gbm` with ``cv.folds=5`` to perform 5-fold cross-validation, and set ``class.stratify.cv`` to ``AHD`` (heart disease outcome) so that cross-validation is performed stratifying by ``AHD``.  Plot the cross-validation error as a function of the boosting iteration/trees (the `$cv.error` component of the object returned by ``gbm``) and determine whether additional boosting iterations are warranted. If so, run additional iterations with  ``gbm.more`` (use the R help to check its syntax). Choose the optimal number of iterations. Use the ``summary.gbm`` function to generate the variable importance plot and extract variable importance/influence (``summary.gbm`` does both). Generate 1D and 2D marginal plots with ``gbm.plot`` to assess the effect of the top three variables and their 2-way interactions. 

```{r}
heart_boost = gbm(AHD ~., data = heart_train, distribution = "bernoulli",
                  n.trees = 3000, interaction.depth = 1, shrinkage = 0.01, 
                  cv.folds = 5,
                  class.stratify.cv = TRUE)


plot(heart_boost$train.error, cex.lab = 2, cex.axis = 2, col = "red",
     type = "l", lwd = 3, ylim = c(0, 1.5), ylab = "error")

lines(heart_boost$cv.error, col = "steelblue", lwd = 3)
```


---


## Question 4: Gradient Boosting

Evaluate the effect of critical boosting parameters (number of boosting iterations, shrinkage/learning rate, and tree depth/interaction).  In ``gbm`` the number of iterations is controlled by ``n.trees`` (default is 100), the shrinkage/learning rate is controlled by ``shrinkage`` (default is 0.001), and interaction depth by ``interaction.depth`` (default is 1).

Note, boosting can overfit if the number of trees is too large. The shrinkage parameter controls the rate at which the boosting learns. Very small $\lambda$ can require using a very large number of trees to achieve good performance. Finally, interaction depth controls the interaction order of the boosted model. A value of 1 implies an additive model, a value of 2 implies a model with up to 2-way interactions, etc. the default is 1.


- Set the seed and train a boosting classification with ``gbm`` using 10-fold cross-validation (``cv.folds=10``) on the training data with ``n.trees = 5000``, ``shrinkage = 0.001``, and ``interaction.depth =1``. Plot the cross-validation errors as a function of the boosting iteration and calculate the test MSE.

```{r}
set.seed(301)
heart_boost1 = gbm(AHD ~ ., data = heart[train, ], distribution = "bernoulli", n.trees = 5000, interaction.depth = 1, shrinkage = 0.001, cv.folds = 10, class.stratify.cv = TRUE)

summary(heart_boost1)

#calculate MSE
min(heart_boost1$cv.error)
yhat_boost<-predict(heart_boost1, newdata=heart_test, n.trees = 5000)
mean((yhat_boost - heart_test$AHD)^2)

plot(heart_boost1$train.error, cex.lab = 2, cex.axis = 2, col = "red",
     type = "l", lwd = 3, ylim = c(0, 1.5), ylab = "error")

lines(heart_boost1$cv.error, col = "steelblue", lwd = 3)
```


- Repeat the above using the same seed and ``n.trees=5000`` with the following 3 additional combination of parameters: 
a) ``shrinkage = 0.001``, ``interaction.depth = 2``; 
b) ``shrinkage = 0.01``, ``interaction.depth = 1``; 
c) ``shrinkage = 0.01``, ``interaction.depth = 2``.
```{r}
set.seed(301)
heart_boost_a = gbm(AHD ~ ., data = heart[train, ], distribution = "bernoulli", n.trees = 5000, interaction.depth = 2, shrinkage = 0.001, cv.folds = 10, class.stratify.cv = TRUE)

summary(heart_boost_a)

#calculate MSE
min(heart_boost_a$cv.error)
yhat_boost<-predict(heart_boost_a, newdata=heart_test, n.trees = 5000)
mean((yhat_boost - heart_test$AHD)^2)

plot(heart_boost_a$train.error, cex.lab = 2, cex.axis = 2, col = "red",
     type = "l", lwd = 3, ylim = c(0, 1.5), ylab = "error")

lines(heart_boost_a$cv.error, col = "steelblue", lwd = 3)
```


```{r}
set.seed(301)
heart_boost_b = gbm(AHD ~ ., data = heart[train, ], distribution = "bernoulli", n.trees = 5000, interaction.depth = 1, shrinkage = 0.01, cv.folds = 10, class.stratify.cv = TRUE)

summary(heart_boost_b)

#calculate MSE
min(heart_boost_b$cv.error)
yhat_boost2<-predict(heart_boost_b, newdata=heart_test, n.trees = 5000)
mean((yhat_boost2 - heart_test$AHD)^2)

plot(heart_boost_b$train.error, cex.lab = 2, cex.axis = 2, col = "red",
     type = "l", lwd = 3, ylim = c(0, 1.5), ylab = "error")

lines(heart_boost_b$cv.error, col = "steelblue", lwd = 3)
```
```{r}
set.seed(301)
heart_boost_c = gbm(AHD ~ ., data = heart[train, ], distribution = "bernoulli", n.trees = 5000, interaction.depth = 2, shrinkage = 0.01, cv.folds = 10, class.stratify.cv = TRUE)

summary(heart_boost_c)

#calculate MSE
min(heart_boost_c$cv.error)
yhat_boost3<-predict(heart_boost_c, newdata=heart_test, n.trees = 5000)
mean((yhat_boost3 - heart_test$AHD)^2)

plot(heart_boost_c$train.error, cex.lab = 2, cex.axis = 2, col = "red",
     type = "l", lwd = 3, ylim = c(0, 1.5), ylab = "error")

lines(heart_boost_c$cv.error, col = "steelblue", lwd = 3)
```

## Question 5: Extreme Gradient Boosting

Train a XGBoost model with `xgboost` and perform a grid search for tuning the number of trees and the maximum depth of the tree. Also perform 10-fold cross-validation and determine the variable importance. Finally, compute the test MSE.

Tuning parameters
- max_depth: tree depth, larger makes model more complex and potentially overfit
- nrounds: number of boosting iterations
- eta: learning rate (shrinkage)
- gamma: minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be (simpler model)
- min_child_weight: controls the minimum number of samples in a leaf node before further splitting
- colsample_bytree: controls the fraction of features (variables) used to build each tree. Default is 1 which is all features

```{r}

train_control = trainControl(method = "cv", number = 10, search ="grid")

tune_grid<-  expand.grid(max_depth = c(1, 3, 5, 7), 
                        nrounds = (1:10)*50, 
                        eta = c(0.01,0.1,0.3), 
                        gamma = 0, 
                        subsample = 1,
                        min_child_weight = 1,
                        colsample_bytree = 0.6 
                        )

heart_xgb<-caret::train(
  AHD ~.,, 
  data = heart_train,
  method="xgbTree",
trControl = train_control, 
tuneGrid = tune_grid, 
na.action = na.exclude,
verbosity = 0
)

varimp <- varImp(heart_xgb, scale = FALSE)
plot(varimp)
yhat_xgb<-predict(heart_xgb, newdata=heart_test)
mean((yhat_xgb-heart_test$AHD)^2)
caret::RMSE(heart_test$AHD, yhat_xgb)

```

- Compare the the performance of the different models and summarize

* The XGBoost model outperforms the GBM model. It outperforms the GBM model for all combinations/tunings of parameters, because the XGBoost model achieves a much lower MSE (0.221) and RMSE (0.47), indicating better predictive accuracy. Though, there still can be improvements in all models. 



