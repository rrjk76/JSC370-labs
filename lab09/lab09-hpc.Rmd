---
title: "Lab 9 - HPC"
output: 
html_document: default
link-citations: yes
---

# Learning goals

In this lab, you are expected to practice the following skills:

- Evaluate whether a problem can be parallelized or not.
- Practice with the parallel package.
- Use Rscript to submit jobs.

```{r eval=FALSE, echo=FALSE}
# install any missing packages
install.packages("microbenchmark")
```

## Problem 1

Give yourself a few minutes to think about what you learned about parallelization. List three
examples of problems that you believe may be solved using parallel computing,
and check for packages on the HPC CRAN task view that may be related to it.

- Cross-Validation in machine learning - distributing training 
- caret package -> supports parallel cross-validation with `doParallel` 
- mlr, foreach, doParallel -> for parallel model training 

- Bootstrapping (like for estimating confidence intervals, drawing random samples)
- boot -> for bootstrapping 
- parallel -> parallelize resampling 
- doParallel, foreach 

- Bayesian Inference: 
- markove chain monte carlo (when approximating distributions and generating samples from the posterior distributions)  
- parallel
-rstan -> for stan in bayesian modelling 
- RcppParallel -> parallel mcmc sampling automatically
-nimle -> customize bayesian inference 


## Problem 2: Pre-parallelization

The following functions can be written to be more efficient without using
`parallel`:

1. This function generates a `n x k` dataset with all its entries having a Poisson distribution with mean `lambda`.

```{r p2-fun1}
# inefficient because it's using a for loop with rbind (reallocating memory each time). this wastes a lot of computation power when reallocating memory (worse with larger matrices). 
# using rbind is very slow, because R copies every object at every iteration 
fun1 <- function(n = 100, k = 4, lambda = 4) {
  x <- NULL
  
  for (i in 1:n)
    x <- rbind(x, rpois(k, lambda))
  
  return(x)
}

fun1alt <- function(n = 100, k = 4, lambda = 4) {
  # YOUR CODE HERE
  matrix(rpois(n*k, lambda = lambda), ncol = k)
  # here, we're using matrix function instead of for loop and rbind
  # matrix allows us to pre-allocate memory. 
  # rpois(n*k, lambda = lambda) allows us to generate the random numbers all at once 
}

# Benchmarking
microbenchmark::microbenchmark(
  fun1(100),
  fun1alt(100),
  unit = "ns"
)
```

How much faster?
_Much faster in every term, implying that the new function is more efficient. After some calculations, I found that fun1alt(100) is approximately 8.88 times faster than fun1(100)._


2.  Find the column max (hint: Checkout the function `max.col()`).

```{r p2-fun2}
# Data Generating Process (10 x 10,000 matrix)
set.seed(1234)
x <- matrix(rnorm(1e4), nrow=10)

# Find each column's max value
fun2 <- function(x) {
  apply(x, 2, max)
}

fun2alt <- function(x) {
  # YOUR CODE HERE
  x[cbind(max.col(t(x)), 1:ncol(x))]
  # avoids function calls inside loops
  # directly extracts the max values 
}

# Benchmarking
bench <- microbenchmark::microbenchmark(
  fun2(x),
  fun2alt(x),
  unit = "micro"
)
```

- inefficient, b/c it loops through all columns and applies the max function, which leads to a slower run time. (apply function is slower than direct matrix indexing)
_Answer here with a plot._
```{r}
# plot 
plot(bench)
ggplot2:: autoplot(bench) + 
  ggplot2::theme_bw()
```


## Problem 3: Parallelize everything

We will now turn our attention to non-parametric 
[bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)).
Among its many uses, non-parametric bootstrapping allow us to obtain confidence
intervals for parameter estimates without relying on parametric assumptions.

The main assumption is that we can approximate many experiments by resampling
observations from our original dataset, which reflects the population. 

This function implements the non-parametric bootstrap:

```{r p3-boot-fun, eval = FALSE}
library(parallel)
my_boot <- function(dat, stat, R, ncpus = 1L) {
  
  # Getting the random indices
  n <- nrow(dat)
  idx <- matrix(sample.int(n, n*R, TRUE), nrow=n, ncol=R)
 
  # Making the cluster using `ncpus`
  # STEP 1: GOES HERE
  # creates a cluster for parallel computer: 
  # ncpus specifies using multiple CPU cores 
  # PSOCK = parallel socket cluster 
  # need a cluster, b/c R by default runs sequentially. By creating clusters, it allows us     to run multiple independent computations, running many across the CPU ports 
  cl <- makePSOCKcluster(ncpus)
  
  # STEP 2: GOES HERE
  # on.exit(stopCluster(cl)) automatically shuts down clusters. w/o this, the clusters        continue, takes up memory, and could cause memory leaks 
  # export the variables to the cluster 
  
  clusterExport(cl, varlist = c("idx", "dat", "stat"), envir = environment()) 
    # sending variables to all worker nodes
    # each run in isolated environments, don't have access to global variable 
    # idx -> resampling indices for bootstrap 
    # dat -> dataset 
    # stat -> the statistical function that we use to compute the estimates 
  
  
  # change sequential apply to parallelized apply
  # STEP 3: THIS FUNCTION NEEDS TO BE REPLACED WITH parLapply
  ans <- parLapply(cl, seq_len(R), function(i) {
    stat(dat[idx[,i], , drop=FALSE])
  })
  
  # Coercing the list into a matrix
  ans <- do.call(rbind, ans)
  
  # STEP 4: GOES HERE
  stopCluster(cl)
  # why? 
    # to free up the system resources 
  
  ans
  
}
```

1. Use the previous pseudocode, and make it work with `parallel`. Here is just an example for you to try:

```{r p3-test-boot, eval = FALSE}
# Bootstrap of a linear regression model
my_stat <- function(d)  coef(lm(y~x, data = d))

# DATA SIM
set.seed(1)
n <- 500 
R <- 1e4
x <- cbind(rnorm(n)) 
y <- x*5 + rnorm(n)

# Check if we get something similar as lm
# OLS CI 
ans0 <- confint(lm(y~x))
cat("OLS CI \n")
print(ans0)

ans1 <- my_boot(dat = data.frame(x, y), my_stat, R = R, ncpus = 4)
qs <- c(.025, .975)
cat("Bootsrap CI \n")
print(t(apply(ans1, 2, quantile, probs = qs)))
```

2. Check whether your version actually goes faster than the non-parallel version:

```{r benchmark-problem3, eval = FALSE}
# your code here
parallel::detectCores()

# non-parallel 1 core
system.time(my_boot(dat = data.frame(x, y), my_stat, R = R, ncpus = 1L))


# parallel 8 cores
system.time(my_boot(dat = data.frame(x, y), my_stat, R = R, ncpus = 8L))
```

_Yes, my version does actually go faster than the non-parallel version. This is seen, as the 8 core one runs much faster than the 1 core one. As seen above, the elapsed time for the parallel 8 cores is shorter than the elapsed time for the non-parallel 1 core (3.134 > 1.743)_

## Problem 4: Compile this markdown document using Rscript

Once you have saved this Rmd file, try running the following command
in your terminal:

```bash
Rscript --vanilla -e 'rmarkdown::render("[full-path-to-your-Rmd-file.Rmd]")' &
```

Where `[full-path-to-your-Rmd-file.Rmd]` should be replace with the full path to
your Rmd file... :).


